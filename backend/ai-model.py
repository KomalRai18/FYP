# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o8LyAury84XpnIPy5jbPk7JCIPxX_auj
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Load datasets
fake_df = pd.read_csv('/content/drive/MyDrive/FakeNewsProject/Fake.csv')
true_df = pd.read_csv('/content/drive/MyDrive/FakeNewsProject/True.csv')

# Add labels
fake_df['label'] = 1  # 1 for fake news
true_df['label'] = 0  # 0 for real news

# Combine datasets
df = pd.concat([fake_df, true_df], axis=0)

# Shuffle the dataset
df = df.sample(frac=1).reset_index(drop=True)

# Display first few rows
df.head()

# Check for missing values
print("Missing values:\n", df.isnull().sum())

# Drop rows with missing values if any
df = df.dropna()

# Combine title and text for better analysis
df['content'] = df['title'] + ' ' + df['text']

# Text cleaning function
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def clean_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text, re.I|re.A)
    # Tokenize
    tokens = text.split()
    # Remove stopwords and lemmatize
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    # Join back to string
    return ' '.join(tokens)

# Apply cleaning
df['clean_content'] = df['content'].apply(clean_text)

# Display cleaned text sample
print("\nOriginal text:\n", df['content'][0])
print("\nCleaned text:\n", df['clean_content'][0])

# Plot label distribution
plt.figure(figsize=(8,6))
sns.countplot(x='label', data=df)
plt.title('Distribution of Real vs Fake News')
plt.xlabel('Label (0:Real, 1:Fake)')
plt.ylabel('Count')
plt.show()

# Word cloud for fake news
from wordcloud import WordCloud

fake_text = " ".join(df[df['label']==1]['clean_content'])
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(fake_text)
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.title('Common words in Fake News')
plt.axis('off')
plt.show()

# Word cloud for real news
real_text = " ".join(df[df['label']==0]['clean_content'])
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(real_text)
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.title('Common words in Real News')
plt.axis('off')
plt.show()

# Split data into train and test
X = df['clean_content']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenization
max_words = 10000
max_len = 300

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)

# Convert text to sequences
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Pad sequences
X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)

print("Training shape:", X_train_pad.shape)
print("Testing shape:", X_test_pad.shape)

# Updated model building cell
from tensorflow.keras import layers

# Build LSTM model - updated version without input_length
model = Sequential([
    layers.Embedding(input_dim=max_words, output_dim=128),  # Removed input_length
    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),
    layers.Dropout(0.2),
    layers.Bidirectional(layers.LSTM(32)),
    layers.Dropout(0.2),
    layers.Dense(24, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# The model will build itself when it first sees data
model.build(input_shape=(None, max_len))  # Explicitly build the model

model.summary()

# Define early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)

# Train model
history = model.fit(
    X_train_pad,
    y_train,
    epochs=10,
    batch_size=128,
    validation_split=0.1,
    callbacks=[early_stop]
)

# Plot training history
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()

plt.show()

# Evaluate on test set
test_loss, test_acc = model.evaluate(X_test_pad, y_test)
print(f'\nTest Accuracy: {test_acc:.4f}')

# Predictions
y_pred = (model.predict(X_test_pad) > 0.5).astype("int32")

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Real', 'Fake'],
            yticklabels=['Real', 'Fake'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Classification report
print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))

# Save model
model.save('/content/drive/MyDrive/fake_news_detector.h5')

# Save tokenizer
import pickle
with open('/content/drive/MyDrive/tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

print("Model and tokenizer saved successfully!")

# Function to predict custom input
def predict_news(text):
    # Clean the text
    text = clean_text(text)
    # Tokenize and pad
    seq = tokenizer.texts_to_sequences([text])
    pad = pad_sequences(seq, maxlen=max_len)
    # Predict
    pred = model.predict(pad)
    # Return result
    return "Fake News" if pred > 0.5 else "Real News"

# Test with sample text
sample_real = "Scientists have discovered a new species of dinosaur in Argentina. The fossilized remains suggest..."
sample_fake = "Breaking: Alien spaceship spotted over White House! Government confirms extraterrestrial visit..."

print("Sample Real News Prediction:", predict_news(sample_real))
print("Sample Fake News Prediction:", predict_news(sample_fake))

# Save model in the new Keras format
model.save('/content/drive/MyDrive/fake_news_detector.keras')

# Save tokenizer (unchanged)
import pickle
with open('/content/drive/MyDrive/tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

print("Model and tokenizer saved successfully in modern format!")

import tensorflow as tf
import pickle
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk

# Download NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize text cleaning tools
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Load model and tokenizer
def load_resources():
    model = tf.keras.models.load_model('/content/drive/MyDrive/fake_news_detector.keras')
    with open('/content/drive/MyDrive/tokenizer.pickle', 'rb') as handle:
        tokenizer = pickle.load(handle)
    return model, tokenizer

# Text cleaning function
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text, re.I|re.A)
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Prediction function
def predict_news(model, tokenizer, text, max_len=300):
    cleaned_text = clean_text(text)
    sequence = tokenizer.texts_to_sequences([cleaned_text])
    padded = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=max_len)
    prediction = model.predict(padded)
    return "Fake News" if prediction > 0.5 else "Real News"

# Main interactive function
def interactive_predictor():
    print("\n" + "="*50)
    print("FAKE NEWS DETECTOR".center(50))
    print("="*50)
    print("\nEnter news text to analyze (type 'quit' to exit)\n")

    model, tokenizer = load_resources()

    while True:
        user_input = input("\nNews Text: ")
        if user_input.lower() == 'quit':
            print("\nExiting Fake News Detector...")
            break

        if len(user_input.split()) < 5:  # Minimum word check
            print("Please enter more text for accurate prediction (at least 5 words)")
            continue

        prediction = predict_news(model, tokenizer, user_input)
        print("\n" + "-"*50)
        print(f"PREDICTION: {prediction}")
        print("-"*50 + "\n")

# Run the interactive predictor
if __name__ == "__main__":
    interactive_predictor()

